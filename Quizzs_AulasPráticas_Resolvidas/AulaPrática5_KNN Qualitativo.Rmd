---
title: "MAS | Aula Prática 5 - K-Nearest Neighbour"
author: "André Silvestre Nº104532 CDB1"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Exemplo de classificação sobre dataset `Iris`

- Margarida G. M. S. Cardoso
- 24 de fevereiro, 2023

# PL5 Goal:

\textcolor{lightgray}{To use the K-Nearest Neighbour method to predict Species in the iris data set}

- Usar o método K-Vizinho mais próximo para prever espécies no conjunto de dados da íris

```{r}
# Bibliotecas
rm(list=ls(all=TRUE)) # First remove everything!

library(FNN)          # Para hacer o KNN
```


\newpage

# 1) The iris data set

## 1.1)	Read the data set

```{r}
data(iris)
head(iris)
```

## 1.2)	Correlations between predictors

```{r}
round(cor(iris[,1:4]),3)
```

> Assim verificamos se são ou n ortogonais entre si


## 1.3)	Constitute (stratified) training and test sets

```{r}
set.seed(777)
ind_train <- c(sample(1:50,35), sample(51:100,35), sample(101:150,35))

# training set
iris_train <- iris[ind_train,] 
table(iris_train$Species)

prop.table(table(iris_train$Species))

# test set
iris_test <- iris[-ind_train,] 
table(iris_test$Species)

prop.table(table(iris_test$Species))
```

## 1.4)	Normalize (stratified) training and test sets

```{r}
normalize <- function(x){
  return ((x -min(x)) / (max(x)-min(x)))
  }

# training set
iris_train_n<-iris_train
# Normalização do Conjunto Treino
iris_train_n[,1:4]<-sapply(iris_train[,1:4],normalize)

# test set
iris_test_n<-iris_test
# Normalização do Conjunto Teste
iris_test_n [,1:4]<-sapply(iris_test[,1:4],normalize)
```


\newpage

# 2) k-Nearest neighbour on data set iris with no test set: the problem

```{r}
knn.iris<- knn(iris[,1:4], iris[,1:4], iris$Species, k=1,prob = TRUE)
str(knn.iris)

# The nearest neighbors of observation 1 include the same observation...
attr(knn.iris,"nn.index")[1,]

# hence the corresponding Euclidean distance is zero...
attr(knn.iris,"nn.dist")[1,] 

# results from classification 	
table(knn.iris ,iris$Species)

mean(knn.iris== iris$Species )
# accuracy reveals “perfect” overfitting

```


\newpage

# 3) k-Nearest neighbour with K=2: cross-validation and some insights

```{r}
# KNN
knn.iris<- knn.cv(iris[,1:4], iris$Species, k=2,prob=TRUE, algorithm="brute")

# knn.cv uses leave-one-out cross validation. For each row of the training set train, the k nearest (in Euclidean distance) other training set vectors are found, and the classification is decided by majority vote, with ties broken at random. If there are ties for the kth nearest vector, all candidates are included in the vote.

# The brute algorithm searches linearly. It is a naive method.

#  in this example, for each observation in the test set KNN uses the 2 ne arest neighbours in the training set to provide predictions for Species

#	 # example from setosa #  	
# The nearest neighbors of observation 1

attr(knn.iris,"nn.index")[1,]
attr(knn.iris,"nn.dist")[1,]

# The nearest neighbors distances to observation 1 are Euclidean distance s
dist(as.matrix(rbind(iris[1,-5],iris[18,-5])), method="euclidean")
dist(as.matrix(rbind(iris[1,-5],iris[5,-5])), method="euclidean")

# The nearest neighbors target values
iris$Species[18]
iris$Species[5]

# The nearest neighbors prediction
knn.iris[[1]]
# and the corresponding probability
attr(knn.iris,"prob")[1]

# results from classification
table(knn.iris ,iris$Species) 

#accuracy
mean(knn.iris== iris$Species )
```


# 4)	Using K-Nearest Neighbour with training and test sets and normalized data

## 4.a)	K-Nearest Neighbour with $K=2$

```{r}
knn.iris_train<- knn(iris_train_n[,1:4], iris_test_n[,1:4], iris_train_n$ Species, k=2,prob = TRUE)

# classification results (for test set)
table(knn.iris_train ,iris_test_n$Species)

# Accuracy
mean(knn.iris_train== iris_test_n$Species)
```

## 4.b)	Selecting the "best" k according to accuracy

```{r out.width='70%', fig.align='center'}
k.accuracy<-matrix(NA,25,2)

for (i in 1:25){ # i é o k | PQ 25? pq é prático
  knn.iris_k <- knn.cv(iris_train_n[,1:4], iris_train_n$Species, k=i,prob = FALSE) 
  k.accuracy[i,1]<-i 
  k.accuracy[i,2] <- mean(knn.iris_k == iris_train_n$Species ) 
  }

# Accuracy plot
plot(k.accuracy[,2], type="b", xlab="K Value",ylab="Accuracy")

# Ordenar pela Accuracy
k.accuracy<-k.accuracy[order(k.accuracy[,2],decreasing=TRUE),]

# best k
k.accuracy[1,1]
```

## 4.c)	Classification results, on training set, according to selected $k$

```{r}
knn.iris_train <- knn(iris_train_n[,1:4],iris_test_n[,1:4], iris_train_n$ Species, k=k.accuracy[1,1],prob = FALSE)

#
table(knn.iris_train ,iris_test_n$Species) 
mean(knn.iris_train== iris_test_n$Species) # Accuracy
```

## 4.c)	Classification results, on test set, according to selected $k$

```{r}
knn.iris_test <- knn(iris_train_n[,1:4], iris_test_n[,1:4], iris_train_n$Species, k=k.accuracy[1,1],prob = FALSE) 
# 
table(knn.iris_test ,iris_test_n$Species)

mean(knn.iris_test== iris_test_n$Species )
```

\newpage

# Exercícios

\textcolor{lightgray}{**EXERCISE 1:** Comment on following statement: “The fact the predictors are metric is relevant for this implementation of KNN; furthermore, they should be uncorrelated”}

- Comenta a seguinte afirmação: "O facto de os preditores serem métricos é relevante para esta implementação da KNN; além disso, devem ser não correlacionados"

> Tal como referido relativamente à escala poder enviesar o resultado, o uso de variávis não métricas fazem o mesmo (por exemplo uma vár. binária - 0 e 1 - torna este prefitor preterido, comparando com uma var. númerica de 0 - 100). A solução passa por **normalizar** as variáveis.


---

\textcolor{lightgray}{**EXERCISE 2:** Illustrate the pertinence of normalization considering distances between iris[1,c(3,4)] and nearest neighbours candidates iris[4,c(3,4)]and iris[7,c(3,4)]}

- Ilustrar a pertinência da normalização considerando distâncias entre a íris[1,c(3,4)) e os candidatos vizinhos mais próximos da iris[4,c(3,4)] e a íris[7,c(3,4)]

> Normalization is relevant when dealing with distance metrics that are sensitive to differences in scale between different features. In the case of iris data, the features `Petal Length` and `Petal Width` have different ranges of values, which can cause issues when using distance-based algorithms like k-Nearest Neighbors (k-NN).

```{r}
data(iris)
iris_features <- iris[, c(3, 4)]
iris_features <- as.matrix(iris_features)

# Compute Euclidean distances without normalization
distances <- dist(rbind(iris_features[1,], iris_features[c(4,7),]), method = "euclidean")
distances
```

However, we can see that the scale of the features `Petal Length` and `Petal Width` are different, which can affect the distance calculation. To address this issue, we can normalize the data so that each feature has a mean of 0 and a standard deviation of 1:


```{r}
# Normalize data
normalized_features <- normalize(iris_features)

# Compute Euclidean distances with normalization
distances_normalized <- dist(rbind(normalized_features[1,], normalized_features[c(4,7),]), method = "euclidean")
distances_normalized
```

> The output of the distances_normalized variable shows that the order of the nearest neighbors candidates has changed.